{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ann04ka/Labs/blob/main/notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ue92g-VODiM-"
      },
      "outputs": [],
      "source": [
        "import keras.ops as ops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wwK2UoHv6zsr"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers, Model\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
        "!unzip -q spa-eng.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8HONFvUGuFq",
        "outputId": "3269f38f-0f49-496f-f431-59fa27b24542"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-04 04:02:35--  http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.195.207, 74.125.20.207, 108.177.98.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.195.207|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2638744 (2.5M) [application/zip]\n",
            "Saving to: ‘spa-eng.zip’\n",
            "\n",
            "\rspa-eng.zip           0%[                    ]       0  --.-KB/s               \rspa-eng.zip         100%[===================>]   2.52M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2025-06-04 04:02:35 (212 MB/s) - ‘spa-eng.zip’ saved [2638744/2638744]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "P0urC6qKrupu"
      },
      "outputs": [],
      "source": [
        "text_file = \"spa-eng/spa.txt\"\n",
        "with open(text_file) as f:\n",
        "    lines = f.read().split(\"\\n\")[:-1]\n",
        "text_pairs = []\n",
        "for line in lines:\n",
        "    english, spanish = line.split(\"\\t\")\n",
        "    spanish = \"[start] \" + spanish + \" [end]\"\n",
        "    text_pairs.append((english, spanish))\n",
        "\n",
        "import random\n",
        "random.shuffle(text_pairs)\n",
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n",
        "test_pairs = text_pairs[num_train_samples + num_val_samples:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "oOr5m_4L6zso"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import string\n",
        "import re\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "strip_chars = string.punctuation + \"¿\"\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "\n",
        "def custom_standardization(input_string):\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    return tf.strings.regex_replace(\n",
        "        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n",
        "\n",
        "vocab_size = 15000\n",
        "sequence_length = 20\n",
        "\n",
        "source_vectorization = layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")\n",
        "target_vectorization = layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length + 1,\n",
        "    standardize=custom_standardization,\n",
        ")\n",
        "train_english_texts = [pair[0] for pair in train_pairs]\n",
        "train_spanish_texts = [pair[1] for pair in train_pairs]\n",
        "source_vectorization.adapt(train_english_texts)\n",
        "target_vectorization.adapt(train_spanish_texts)\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "def format_dataset(eng, spa):\n",
        "    eng = source_vectorization(eng)\n",
        "    spa = target_vectorization(spa)\n",
        "    return ({\n",
        "        \"english\": eng,\n",
        "        \"spanish\": spa[:, :-1],\n",
        "    }, spa[:, 1:])\n",
        "\n",
        "def make_dataset(pairs):\n",
        "    eng_texts, spa_texts = zip(*pairs)\n",
        "    eng_texts = list(eng_texts)\n",
        "    spa_texts = list(spa_texts)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(format_dataset, num_parallel_calls=4)\n",
        "    return dataset.shuffle(2048).prefetch(16).cache()\n",
        "\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "embed_dim = 256\n",
        "latent_dim = 1024\n",
        "\n",
        "source = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
        "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(source)\n",
        "encoded_source = layers.Bidirectional(\n",
        "    layers.GRU(latent_dim), merge_mode=\"sum\")(x)\n",
        "\n",
        "past_target = keras.Input(shape=(None,), dtype=\"int64\", name=\"spanish\")\n",
        "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(past_target)\n",
        "decoder_gru = layers.GRU(latent_dim, return_sequences=True)\n",
        "x = decoder_gru(x, initial_state=encoded_source)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "target_next_step = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "seq2seq_rnn = keras.Model([source, past_target], target_next_step)\n",
        "\n",
        "seq2seq_rnn.compile(\n",
        "    optimizer=\"rmsprop\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"])\n",
        "seq2seq_rnn.fit(train_ds, epochs=1, validation_data=val_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZedwVzTYJCv2",
        "outputId": "33046d07-47eb-40f8-a1a5-76c114ae9106"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m216s\u001b[0m 160ms/step - accuracy: 0.1588 - loss: 5.2638 - val_accuracy: 0.1578 - val_loss: 3.8747\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7e38536e30d0>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "B4meCmvzh_fX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad626d08-a5bb-4433-837f-7c6351f5af7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "embed_dim = 256\n",
        "latent_dim = 1024\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.layers import InputLayer\n",
        "\n",
        "rnn_model = Sequential([\n",
        "    InputLayer(input_shape=(sequence_length,), dtype='int64', name='english'),\n",
        "\n",
        "    layers.Embedding(vocab_size, embed_dim, input_length=sequence_length),\n",
        "\n",
        "    layers.Bidirectional(layers.GRU(latent_dim, return_sequences=False)),\n",
        "\n",
        "    layers.Dropout(0.5),\n",
        "\n",
        "    layers.RepeatVector(sequence_length),\n",
        "\n",
        "    layers.GRU(latent_dim * 2, return_sequences=True),\n",
        "\n",
        "    layers.Dropout(0.5),\n",
        "\n",
        "    layers.Dense(vocab_size, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "rnn_model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_model.fit(train_ds, epochs=1, validation_data=train_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGGST0vjKmbL",
        "outputId": "1da7ef23-f795-46d2-e1f5-f0e86d37fed4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py:237: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
            "Expected: english\n",
            "Received: inputs=['Tensor(shape=(None, None))']\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m595s\u001b[0m 454ms/step - accuracy: 0.6700 - loss: 2.3377 - val_accuracy: 0.7221 - val_loss: 1.6060\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7e3878776850>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_model.save(\"rnn_model.keras\")\n",
        "\n",
        "# from tensorflow.keras.models import load_model\n",
        "# seq2seq_rnn = load_model(\"/content/seq2seq_rnn.keras\")"
      ],
      "metadata": {
        "id": "tbTLmPlEMkgI"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "h2uB2De8GZjU"
      },
      "outputs": [],
      "source": [
        "def build_rnn_attention_model():\n",
        "    encoder_inputs = layers.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
        "    x = layers.Embedding(vocab_size, embed_dim)(encoder_inputs)\n",
        "    encoder_gru = layers.GRU(latent_dim, return_sequences=True, return_state=True)\n",
        "    encoder_outputs, encoder_state = encoder_gru(x)\n",
        "\n",
        "    decoder_inputs = layers.Input(shape=(None,), dtype=\"int64\", name=\"spanish\")\n",
        "    y = layers.Embedding(vocab_size, embed_dim)(decoder_inputs)\n",
        "    decoder_gru = layers.GRU(latent_dim, return_sequences=True, return_state=True)\n",
        "    decoder_outputs, _ = decoder_gru(y, initial_state=encoder_state)\n",
        "\n",
        "    # Сохраняем attention layer как отдельный слой\n",
        "    attention_layer = layers.Attention(name=\"attention\")\n",
        "    attention_output = attention_layer([encoder_outputs, decoder_outputs])\n",
        "\n",
        "    merged = layers.Concatenate()([decoder_outputs, attention_output])\n",
        "    merged = layers.Dropout(0.5)(merged)\n",
        "    final_output = layers.Dense(vocab_size, activation=\"softmax\")(merged)\n",
        "\n",
        "    model = Model([encoder_inputs, decoder_inputs], final_output)\n",
        "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "    # Вспомогательная модель для получения attention weights\n",
        "    attention_model = Model(\n",
        "        [encoder_inputs, decoder_inputs],\n",
        "        [final_output, attention_output]\n",
        "    )\n",
        "\n",
        "    return model, attention_model\n",
        "\n",
        "# Построение модели\n",
        "seq2seq_rnn_with_attention, attention_model = build_rnn_attention_model()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_with_attention_model.fit(train_ds, epochs=1, validation_data=train_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "id": "5No2ehC5LePH",
        "outputId": "9e449c88-92b3-4961-8cf4-e678996c0d41"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OperatorNotAllowedInGraphError",
          "evalue": "Exception encountered when calling GRU.call().\n\n\u001b[1mIterating over a symbolic `tf.Tensor` is not allowed. You can attempt the following resolutions to the problem: If you are running in Graph mode, use Eager execution mode or decorate this function with @tf.function. If you are using AutoGraph, you can try decorating this function with @tf.function. If that does not work, then you may be using an unsupported feature or your source code may not be visible to AutoGraph. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code for more information.\u001b[0m\n\nArguments received by GRU.call():\n  • sequences=tf.Tensor(shape=(None, None, 256), dtype=float32)\n  • initial_state=None\n  • mask=None\n  • training=True",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-ed5a374ded75>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrnn_with_attention_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0merror_handler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m: Exception encountered when calling GRU.call().\n\n\u001b[1mIterating over a symbolic `tf.Tensor` is not allowed. You can attempt the following resolutions to the problem: If you are running in Graph mode, use Eager execution mode or decorate this function with @tf.function. If you are using AutoGraph, you can try decorating this function with @tf.function. If that does not work, then you may be using an unsupported feature or your source code may not be visible to AutoGraph. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code for more information.\u001b[0m\n\nArguments received by GRU.call():\n  • sequences=tf.Tensor(shape=(None, None, 256), dtype=float32)\n  • initial_state=None\n  • mask=None\n  • training=True"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "QKCfzcx36zsn"
      },
      "outputs": [],
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.position_embeddings = layers.Embedding(input_dim=sequence_length, output_dim=embed_dim)\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_pos = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_pos\n",
        "\n",
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = tf.keras.Sequential([\n",
        "            layers.Dense(dense_dim, activation=\"relu\"),\n",
        "            layers.Dense(embed_dim)\n",
        "        ])\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]\n",
        "        attn_output = self.attention(inputs, inputs, attention_mask=mask)\n",
        "        out1 = self.layernorm_1(inputs + attn_output)\n",
        "        out2 = self.layernorm_2(out1 + self.dense_proj(out1))\n",
        "        return out2\n",
        "\n",
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.attention_1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.attention_2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = tf.keras.Sequential([\n",
        "            layers.Dense(dense_dim, activation=\"relu\"),\n",
        "            layers.Dense(embed_dim)\n",
        "        ])\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "        else:\n",
        "            padding_mask = causal_mask\n",
        "        attn_output_1 = self.attention_1(inputs, inputs, attention_mask=padding_mask)\n",
        "        out1 = self.layernorm_1(inputs + attn_output_1)\n",
        "        attn_output_2 = self.attention_2(query=out1, value=encoder_outputs, key=encoder_outputs)\n",
        "        out2 = self.layernorm_2(out1 + attn_output_2)\n",
        "        out3 = self.layernorm_3(out2 + self.dense_proj(out2))\n",
        "        return out3\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, seq_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(seq_length)[:, tf.newaxis]\n",
        "        j = tf.range(seq_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        causal_mask = tf.broadcast_to(mask, shape=[batch_size, seq_length, seq_length])\n",
        "        return causal_mask\n",
        "\n",
        "encoder_inputs = layers.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
        "x = TransformerEncoder(embed_dim, 2048, 8)(x)\n",
        "encoder_outputs = x\n",
        "\n",
        "decoder_inputs = layers.Input(shape=(None,), dtype=\"int64\", name=\"spanish\")\n",
        "y = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
        "y = TransformerDecoder(embed_dim, 2048, 8)(y, encoder_outputs)\n",
        "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(y)\n",
        "\n",
        "transformer = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "transformer.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.fit(train_ds, epochs=1, validation_data=train_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfL6d5R2NdeL",
        "outputId": "762a9489-1cbf-4370-c238-243d066b4e3a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1302/1302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 74ms/step - accuracy: 0.6974 - loss: 2.3272 - val_accuracy: 0.7437 - val_loss: 1.5188\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7e38782a5610>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.save(\"transformer.keras\")"
      ],
      "metadata": {
        "id": "zfdYmJvENev5"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "tMRfu7HO6zss"
      },
      "outputs": [],
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "spa_vocab = target_vectorization.get_vocabulary()\n",
        "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
        "\n",
        "def decode_sequence(model, input_sentence):\n",
        "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
        "    decoded_sentence = \"[start]\"\n",
        "    for i in range(sequence_length):\n",
        "        tokenized_target_sentence = target_vectorization([decoded_sentence])[:, :-1]\n",
        "        predictions = model.predict([tokenized_input_sentence, tokenized_target_sentence], verbose=0)\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        sampled_word = spa_index_lookup[sampled_token_index]\n",
        "        if sampled_word == \"[end]\" or len(decoded_sentence.split()) > sequence_length:\n",
        "            break\n",
        "        decoded_sentence += \" \" + sampled_word\n",
        "    return decoded_sentence.replace(\"[start] \", \"\").replace(\" [end]\", \"\")\n",
        "\n",
        "def evaluate_model(model, pairs, n_samples=10):\n",
        "    total_bleu = 0\n",
        "    for _ in range(n_samples):\n",
        "        input_sentence, true_sentence = random.choice(pairs)\n",
        "        pred_sentence = decode_sequence(model, input_sentence)\n",
        "        reference = [true_sentence.replace(\"[start] \", \"\").replace(\" [end]\", \"\").split()]\n",
        "        candidate = pred_sentence.split()\n",
        "        bleu = sentence_bleu(reference, candidate)\n",
        "        total_bleu += bleu\n",
        "    return total_bleu / n_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "9CkQTPIK6zss",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "354772a6-162d-40f3-8e7f-837a9fff98bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU RNN: 6.260842608366016e-232\n",
            "BLEU Transformer: 4.5293359739455395e-232\n"
          ]
        }
      ],
      "source": [
        "# rnn_model.fit(train_ds, validation_data=val_ds, epochs=1)\n",
        "# seq2seq_rnn_with_attention.fit(train_ds, validation_data=val_ds, epochs=1)\n",
        "# transformer.fit(train_ds, validation_data=val_ds, epochs=1)\n",
        "\n",
        "print(\"BLEU RNN:\", evaluate_model(rnn_model, test_pairs))\n",
        "# print(\"BLEU RNN+Attention:\", evaluate_model(seq2seq_rnn_with_attention, test_pairs))\n",
        "print(\"BLEU Transformer:\", evaluate_model(transformer, test_pairs))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtPmzqFiG-go",
        "outputId": "44926201-b88b-4287-fa05-a576704d5232"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<CacheDataset element_spec=({'english': TensorSpec(shape=(None, None), dtype=tf.int64, name=None), 'spanish': TensorSpec(shape=(None, None), dtype=tf.int64, name=None)}, TensorSpec(shape=(None, None), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "spa_vocab = target_vectorization.get_vocabulary()\n",
        "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
        "eng_vocab = source_vectorization.get_vocabulary()\n",
        "eng_index_lookup = dict(zip(range(len(eng_vocab)), eng_vocab))"
      ],
      "metadata": {
        "id": "h23JbvUAPrjf"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_attention(model, input_sentence):\n",
        "    tokenized_input = source_vectorization([input_sentence])\n",
        "    decoded_sentence = \"[start]\"\n",
        "    all_att_weights = []\n",
        "    generated_words = []\n",
        "\n",
        "    for i in range(20):  # максимум 20 шагов\n",
        "        tokenized_target = target_vectorization([decoded_sentence])[:, :-1]\n",
        "        output, att_weights = model.predict([tokenized_input, tokenized_target], verbose=0)\n",
        "\n",
        "        sampled_token_idx = np.argmax(output[0, i, :])\n",
        "        sampled_word = spa_index_lookup[sampled_token_idx]\n",
        "\n",
        "        if sampled_word == \"[end]\" or len(generated_words) >= 20:\n",
        "            break\n",
        "\n",
        "        decoded_sentence += \" \" + sampled_word\n",
        "        generated_words.append(sampled_word)\n",
        "        all_att_weights.append(att_weights[0][i])\n",
        "\n",
        "    input_words = input_sentence.split()\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    sns.heatmap(\n",
        "        np.array(all_att_weights),\n",
        "        xticklabels=input_words,\n",
        "        yticklabels=generated_words,\n",
        "        cmap=\"Blues\",\n",
        "        annot=True,\n",
        "        fmt=\".2f\",\n",
        "        ax=ax\n",
        "    )\n",
        "    ax.set_xlabel(\"Input Words (English)\")\n",
        "    ax.set_ylabel(\"Generated Translation (Spanish)\")\n",
        "    ax.set_title(\"Word Alignment via Attention\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "zc4z5XSJPqUx"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention_model = Model(\n",
        "    inputs=[source, past_target],\n",
        "    outputs=[rnn_model.output, att_layer_output]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "-xi98OQuSRfW",
        "outputId": "c8a151b3-673e-423b-a502-0c8d96d0bc93"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "The layer sequential_6 has never been called and thus has no defined output.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-2c4fde1133bf>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m attention_model = Model(\n\u001b[1;32m      2\u001b[0m     \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast_target\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrnn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matt_layer_output\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/ops/operation.py\u001b[0m in \u001b[0;36moutput\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0mOutput\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0moutput\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \"\"\"\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_node_attribute_at_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_tensors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_node_attribute_at_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/ops/operation.py\u001b[0m in \u001b[0;36m_get_node_attribute_at_index\u001b[0;34m(self, node_index, attr, attr_name)\u001b[0m\n\u001b[1;32m    283\u001b[0m         \"\"\"\n\u001b[1;32m    284\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inbound_nodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m             raise AttributeError(\n\u001b[0m\u001b[1;32m    286\u001b[0m                 \u001b[0;34mf\"The layer {self.name} has never been called \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m                 \u001b[0;34mf\"and thus has no defined {attr_name}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: The layer sequential_6 has never been called and thus has no defined output."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_sentence, _ = random.choice(test_pairs)\n",
        "visualize_attention(rnn_model, example_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "lkBZHIXHPpGL",
        "outputId": "a80e2728-bafc-4cbb-a003-0973630400f5"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "not enough values to unpack (expected 2, got 1)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-bf1f1f07a234>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mexample_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_pairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvisualize_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-37-3e36e9332ee3>\u001b[0m in \u001b[0;36mvisualize_attention\u001b[0;34m(model, input_sentence)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# максимум 20 шагов\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mtokenized_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_vectorization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdecoded_sentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matt_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokenized_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenized_target\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0msampled_token_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vLUjG7FrPn7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model_by_length(model, pairs, max_len_groups=[10, 20]):\n",
        "    results = {}\n",
        "    short, medium, long = [], [], []\n",
        "\n",
        "    for pair in pairs:\n",
        "        src, tgt = pair\n",
        "        length = len(src.split())\n",
        "        if length <= max_len_groups[0]:\n",
        "            short.append(pair)\n",
        "        elif length <= max_len_groups[1]:\n",
        "            medium.append(pair)\n",
        "        else:\n",
        "            long.append(pair)\n",
        "\n",
        "    def get_bleu(group):\n",
        "        return evaluate_model(model, group, n_samples=10)\n",
        "\n",
        "    results[\"short\"] = get_bleu(short) if short else 0\n",
        "    results[\"medium\"] = get_bleu(medium) if medium else 0\n",
        "    results[\"long\"] = get_bleu(long) if long else 0\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "peEtQIUbHubb"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Оценка по длине:\")\n",
        "for name, model in [\n",
        "    (\"RNN\", rnn_model),\n",
        "    # (\"RNN+Attention\", seq2seq_rnn_with_attention),\n",
        "    (\"Transformer\", transformer)\n",
        "]:\n",
        "    print(f\"\\n{name}:\")\n",
        "    scores = evaluate_model_by_length(model, test_pairs)\n",
        "    print(f\"Short: {scores['short']:.4f}\")\n",
        "    print(f\"Medium: {scores['medium']:.4f}\")\n",
        "    print(f\"Long: {scores['long']:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxDvC6m9RSsm",
        "outputId": "3684c554-902d-4efc-d82a-6abf4e963ab9"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Оценка по длине:\n",
            "\n",
            "RNN:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Short: 0.0000\n",
            "Medium: 0.0000\n",
            "Long: 0.0000\n",
            "\n",
            "Transformer:\n",
            "Short: 0.0000\n",
            "Medium: 0.0000\n",
            "Long: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_long_sentences(n=5, repeat_times=3):\n",
        "    new_pairs = []\n",
        "    for _ in range(n):\n",
        "        pair = random.choice(train_pairs)\n",
        "        src, tgt = pair\n",
        "        src = \" \".join(src.split() * repeat_times)\n",
        "        tgt = \" \".join(tgt.replace(\"[start] \", \"\").replace(\" [end]\", \"\").split() * repeat_times)\n",
        "        new_pairs.append((src, \"[start] \" + tgt + \" [end]\"))\n",
        "    return new_pairs\n",
        "\n",
        "long_test_pairs = test_pairs + generate_long_sentences(n=10, repeat_times=3)"
      ],
      "metadata": {
        "id": "WqpuwjtURXmS"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EAQAB1jiRh7l"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}