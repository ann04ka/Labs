{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ann04ka/Labs/blob/main/Lab2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Реализовать с помощью `Numpy` класс `MyMLP`, моделирующий работу полносвязной нейронной сети.\n",
        "\n",
        "Реализуемый класс должен\n",
        "\n",
        "1. Поддерживать создание любого числа слоев с любым числом нейронов. Тип инициализации весов не регламентируется.\n",
        "2. Обеспечивать выбор следующих функции активации в рамках каждого слоя: `ReLU`, `sigmoid`, `linear`.\n",
        "3. Поддерживать решение задачи классификации и регрессии (выбор соответствующего лосса, в том числе для задачи многоклассовой классификации).\n",
        "4. В процессе обучения использовать самостоятельно реализованный механизм обратного распространения (вывод формул в формате markdown) для применения градиентного и стохастического градиентного спусков (с выбором размера батча)\n",
        "5. Поддерживать использование `l1`, `l2` и `l1l2` регуляризаций."
      ],
      "metadata": {
        "id": "GnNzcz8_ZjVO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Самостоятельно выбрать наборы данных (классификация и регрессия). Провести эксперименты (различные конфигурации сети: количество слоев, нейронов, функции активации, скорость обучения и тп. — минимум 5 различных конфигураций) и сравнить результаты работы (оценка качества модели + время обучения и инференса) реализованного класса `MyMLP` со следующими моделям (в одинаковых конфигурациях):\n",
        "\n",
        "*   MLPClassifier/MLPRegressor из sklearn\n",
        "*   TensorFlow\n",
        "*   Keras\n",
        "*   PyTorch\n",
        "\n",
        "Результат представить в виде .ipynb блокнота, содержащего весь необходимый код и визуализации сравнения реализаций для рассмотренных конфигураций.\n"
      ],
      "metadata": {
        "id": "xTRKg7mlcJBL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MyMLP"
      ],
      "metadata": {
        "id": "cbvj7b3t_OLi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import fetch_california_housing, load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, accuracy_score"
      ],
      "metadata": {
        "id": "SzrX1qlB-81d"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    s = sigmoid(x)\n",
        "    return s * (1 - s)\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return (x > 0).astype(float)\n",
        "\n",
        "def linear(x):\n",
        "    return x\n",
        "\n",
        "def linear_derivative(x):\n",
        "    return np.ones_like(x)\n",
        "\n",
        "def softmax(x):\n",
        "    exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return exps / np.sum(exps, axis=1, keepdims=True)"
      ],
      "metadata": {
        "id": "KIigwN4y-89f"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyMLP:\n",
        "    def __init__(self, layers, activation='relu', output_activation='linear',\n",
        "                 loss='mse', learning_rate=0.01, batch_size=32, epochs=10,\n",
        "                 l2=0.0):\n",
        "        \"\"\"\n",
        "        :param layers: список чисел нейронов в каждом слое, например [8, 64, 1]\n",
        "        :param activation: функция активации скрытых слоёв ('relu'/'sigmoid'/'linear')\n",
        "        :param output_activation: функция активации выходного слоя ('linear'/'softmax')\n",
        "        :param loss: тип лосса ('mse'/'cross_entropy')\n",
        "        :param learning_rate: скорость обучения\n",
        "        :param batch_size: размер батча (исправить)\n",
        "        :param epochs: число эпох\n",
        "        :param l2: коэффициент L2-регуляризации\n",
        "        \"\"\"\n",
        "        self.layers = layers\n",
        "        self.activation_name = activation\n",
        "        self.output_activation_name = output_activation\n",
        "        self.loss_name = loss\n",
        "        self.lr = learning_rate\n",
        "        self.batch_size = batch_size\n",
        "        self.epochs = epochs\n",
        "        self.l2 = l2\n",
        "\n",
        "        self.parameters = {}\n",
        "        for i in range(len(layers) - 1):\n",
        "            self.parameters[f'W{i}'] = np.random.randn(layers[i], layers[i+1]) * 0.01\n",
        "            self.parameters[f'b{i}'] = np.zeros((1, layers[i+1]))\n",
        "\n",
        "        self.activation = {'relu': relu, 'sigmoid': sigmoid, 'linear': linear}[activation]\n",
        "        self.activation_derivative = {'relu': relu_derivative, 'sigmoid': sigmoid_derivative, 'linear': lambda x: 1}[activation]\n",
        "\n",
        "        self.output_activation = {'linear': linear, 'softmax': softmax}[output_activation]\n",
        "\n",
        "        if loss == 'mse':\n",
        "            self.loss_fn = lambda y_true, y_pred: 0.5 * np.mean((y_true - y_pred)**2)\n",
        "            self.dloss_fn = lambda y_true, y_pred: (y_pred - y_true)\n",
        "        elif loss == 'cross_entropy':\n",
        "            self.loss_fn = lambda y_true, y_pred: -np.mean(np.sum(y_true * np.log(y_pred + 1e-15), axis=1))\n",
        "            self.dloss_fn = lambda y_true, y_pred: (y_pred - y_true)\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.A = [X]\n",
        "        self.Z = []\n",
        "        for i in range(len(self.layers) - 1):\n",
        "            W = self.parameters[f'W{i}']\n",
        "            b = self.parameters[f'b{i}']\n",
        "            z = self.A[-1] @ W + b\n",
        "            self.Z.append(z)\n",
        "            a = self.activation(z) if i < len(self.layers) - 2 else self.output_activation(z)\n",
        "\n",
        "            if len(a.shape) == 1:\n",
        "                a = a.reshape(1, -1)\n",
        "\n",
        "            self.A.append(a)\n",
        "        return self.A[-1]\n",
        "\n",
        "    def backward(self, y_true):\n",
        "        grads = {}\n",
        "        m = y_true.shape[0]\n",
        "        dA = self.dloss_fn(y_true, self.A[-1])\n",
        "\n",
        "        for i in reversed(range(len(self.layers) - 1)):\n",
        "            if len(dA.shape) == 1:\n",
        "                dA = dA.reshape(-1, self.layers[i+1])\n",
        "\n",
        "            act_derivative = self.activation_derivative(self.Z[i])\n",
        "            dZ = dA * act_derivative\n",
        "\n",
        "            W = self.parameters[f'W{i}']\n",
        "            dW = self.A[i].T @ dZ / m\n",
        "            db = np.sum(dZ, axis=0, keepdims=True) / m\n",
        "            dA = dZ @ W.T\n",
        "\n",
        "            if self.l2 > 0:\n",
        "                dW += (self.l2 / m) * W\n",
        "\n",
        "            grads[f'dW{i}'] = dW\n",
        "            grads[f'db{i}'] = db\n",
        "\n",
        "        for i in range(len(self.layers) - 1):\n",
        "            self.parameters[f'W{i}'] -= self.lr * grads[f'dW{i}']\n",
        "            self.parameters[f'b{i}'] -= self.lr * grads[f'db{i}']\n",
        "\n",
        "    def fit(self, X_train, y_train):\n",
        "        indices = np.arange(X_train.shape[0])\n",
        "        for epoch in range(self.epochs):\n",
        "            np.random.shuffle(indices)\n",
        "            for i in range(0, X_train.shape[0], self.batch_size):\n",
        "                idx = indices[i:i+self.batch_size]\n",
        "                X_batch, y_batch = X_train[idx], y_train[idx]\n",
        "\n",
        "                y_pred = self.forward(X_batch)\n",
        "                self.backward(y_batch)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.forward(X)"
      ],
      "metadata": {
        "id": "H3lKu2YC-9Aw"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Сравнение"
      ],
      "metadata": {
        "id": "p2obQvkG_YA5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Классификация"
      ],
      "metadata": {
        "id": "Bsa8iINH_izi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "data = load_iris()\n",
        "X_clf, y_clf = data.data, data.target\n",
        "X_clf = StandardScaler().fit_transform(X_clf)\n",
        "\n",
        "ohe = OneHotEncoder(sparse_output=False)\n",
        "y_clf_ohe = ohe.fit_transform(y_clf.reshape(-1, 1))\n",
        "\n",
        "X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(X_clf, y_clf_ohe, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "XKMti_VK-9Dp"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Регрессия"
      ],
      "metadata": {
        "id": "vaomGSec_p7h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = fetch_california_housing()\n",
        "X_reg, y_reg = data.data, data.target\n",
        "X_reg = StandardScaler().fit_transform(X_reg)\n",
        "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "MY_LKWhN-9GZ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Эксперименты с разными конфигурациями\n",
        "\n",
        "Примеры сетей:\n",
        "\n",
        "| Название | Архитектура | Активация | Задача |\n",
        "|---------|-------------|------------|--------|\n",
        "| A       | [64, 32, 1] | ReLU → Linear | Regressor |\n",
        "| B       | [128, 64, 3] | ReLU → Softmax | Classifier |\n",
        "| C       | [64, 64, 3] | ReLU → Softmax | Classifier |\n",
        "| D       | [128, 1]     | Linear       | Regressor |\n",
        "| E       | [64, 32, 3] | Sigmoid → Softmax | Classifier |"
      ],
      "metadata": {
        "id": "Oq_r2bNi_9eS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layer_configurations = {\n",
        "    'regression': {\n",
        "        'MyMLP': [X_train_reg.shape[1], 64, 1],\n",
        "        'sklearn': [64],\n",
        "        'tensorflow': [64],\n",
        "        'keras': [64],\n",
        "        'pytorch': [64]\n",
        "    },\n",
        "    'classification': {\n",
        "        'MyMLP': [X_train_clf.shape[1], 64, y_train_clf.shape[1]],\n",
        "        'sklearn': [64],\n",
        "        'tensorflow': [64],\n",
        "        'keras': [64],\n",
        "        'pytorch': [64]\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "KOX_fr6Q-9JQ"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyMLP(\n",
        "    layers=[X_train_reg.shape[1], 64, 1],\n",
        "    activation='relu',\n",
        "    output_activation='linear',\n",
        "    loss='mse',\n",
        "    learning_rate=0.01,\n",
        "    batch_size=1,\n",
        "    epochs=20\n",
        ")\n",
        "\n",
        "model.fit(X_train_reg, y_train_reg)\n",
        "preds = model.predict(X_test_reg).flatten()"
      ],
      "metadata": {
        "id": "6seHfHqWBuPt"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "print(\"=== РЕГРЕССИЯ ===\")\n",
        "\n",
        "# MyMLP\n",
        "model_mlp_np = MyMLP(\n",
        "    layers=layer_configurations['regression']['MyMLP'],\n",
        "    activation='relu',\n",
        "    output_activation='linear',\n",
        "    loss='mse',\n",
        "    learning_rate=0.01,\n",
        "    batch_size=1,\n",
        "    epochs=20\n",
        ")\n",
        "start_time = time.time()\n",
        "model_mlp_np.fit(X_train_reg, y_train_reg)\n",
        "end_time = time.time()\n",
        "preds_np = model_mlp_np.predict(X_test_reg).flatten()\n",
        "score_np = mean_squared_error(y_test_reg, preds_np)\n",
        "print(f\"MyMLP (NumPy): RMSE={np.sqrt(score_np):.4f}, Time={end_time - start_time:.2f}s\")\n",
        "\n",
        "# sklearn\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "model_sklearn = MLPRegressor(hidden_layer_sizes=layer_configurations['regression']['sklearn'], max_iter=50, random_state=42)\n",
        "start_time = time.time()\n",
        "model_sklearn.fit(X_train_reg, y_train_reg)\n",
        "end_time = time.time()\n",
        "preds_sklearn = model_sklearn.predict(X_test_reg)\n",
        "score_sklearn = mean_squared_error(y_test_reg, preds_sklearn)\n",
        "print(f\"MLPRegressor (sklearn): RMSE={np.sqrt(score_sklearn):.4f}, Time={end_time - start_time:.2f}s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OGCxVTR-9MX",
        "outputId": "707b653c-488a-4ad5-9489-58970551d747"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== РЕГРЕССИЯ ===\n",
            "MyMLP (NumPy): RMSE=0.8908, Time=37.54s\n",
            "MLPRegressor (sklearn): RMSE=0.5766, Time=3.06s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== КЛАССИФИКАЦИЯ ===\")\n",
        "\n",
        "# MyMLP\n",
        "model_mlp_np = MyMLP(\n",
        "    layers=layer_configurations['classification']['MyMLP'],\n",
        "    activation='relu',\n",
        "    output_activation='softmax',\n",
        "    loss='cross_entropy',\n",
        "    learning_rate=0.01,\n",
        "    batch_size=32,\n",
        "    epochs=50\n",
        ")\n",
        "\n",
        "start_time = time.time()\n",
        "model_mlp_np.fit(X_train_clf, y_train_clf)\n",
        "end_time = time.time()\n",
        "preds_np = model_mlp_np.predict(X_test_clf)\n",
        "y_pred_np = np.argmax(preds_np, axis=1)\n",
        "y_true = np.argmax(y_test_clf, axis=1)\n",
        "acc_np = accuracy_score(y_true, y_pred_np)\n",
        "print(f\"MyMLP: Accuracy={acc_np:.4f}, Time={end_time - start_time:.2f}s\")\n",
        "\n",
        "# sklearn\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "model_sklearn = MLPClassifier(hidden_layer_sizes=layer_configurations['classification']['sklearn'],\n",
        "                              max_iter=50, random_state=42)\n",
        "start_time = time.time()\n",
        "model_sklearn.fit(X_train_clf, y_train_clf)\n",
        "end_time = time.time()\n",
        "\n",
        "preds_proba = model_sklearn.predict_proba(X_test_clf)\n",
        "preds_sklearn = np.argmax(preds_proba, axis=1)\n",
        "acc_sklearn = accuracy_score(y_true, preds_sklearn)\n",
        "print(f\"MLPClassifier (sklearn): Accuracy={acc_sklearn:.4f}, Time={end_time - start_time:.2f}s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNcLDs63Az1t",
        "outputId": "ca4a2c87-d406-4385-db54-942c5e7f610b"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== КЛАССИФИКАЦИЯ ===\n",
            "MyMLP: Accuracy=0.3667, Time=0.05s\n",
            "MLPClassifier (sklearn): Accuracy=0.9333, Time=0.05s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# TensorFlow/Keras\n",
        "model_keras = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(X_train_clf.shape[1],)),\n",
        "    Dense(y_train_clf.shape[1], activation='softmax')\n",
        "])\n",
        "model_keras.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "start_time = time.time()\n",
        "model_keras.fit(X_train_clf, y_train_clf, epochs=20, batch_size=1, verbose=0)\n",
        "end_time = time.time()\n",
        "_, acc_keras = model_keras.evaluate(X_test_clf, y_test_clf, verbose=0)\n",
        "print(f\"Keras (TensorFlow): Accuracy={acc_keras:.4f}, Time={end_time - start_time:.2f}s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZK_94Q3iAzy_",
        "outputId": "995b6c08-3485-4855-fec8-2235fa767e46"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keras (TensorFlow): Accuracy=1.0000, Time=7.01s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, output_dim),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "X_train_tensor = torch.tensor(X_train_clf, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train_clf, dtype=torch.float32)\n",
        "train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "net = SimpleNet(X_train_clf.shape[1], 64, y_train_clf.shape[1])\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
        "\n",
        "start_time = time.time()\n",
        "for epoch in range(20):\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, torch.argmax(labels, dim=1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "end_time = time.time()\n",
        "\n",
        "with torch.no_grad():\n",
        "    test_input = torch.tensor(X_test_clf, dtype=torch.float32)\n",
        "    pred = net(test_input).argmax(dim=1)\n",
        "    true = torch.tensor(y_true)\n",
        "    acc_pt = (pred == true).float().mean().item()\n",
        "print(f\"PyTorch: Accuracy={acc_pt:.4f}, Time={end_time - start_time:.2f}s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0msUtALAzwb",
        "outputId": "3c8fbffa-ea0d-41d1-d6b6-7f49dd4e823e"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch: Accuracy=0.9667, Time=4.32s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zrgy0CTwAzt9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Выводы"
      ],
      "metadata": {
        "id": "stt8lDHTIhHQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "| Модель | Задача | Accuracy / RMSE | Время обучения |\n",
        "|-------|--------|------------------|----------------|\n",
        "| **MyMLP (NumPy)** | Регрессия | RMSE = 0.8908 | 37.54 с |\n",
        "| **MLPRegressor (sklearn)** | Регрессия | RMSE = 0.5766 | 3.06 с |\n",
        "| **MyMLP (NumPy)** | Классификация | Accuracy = 0.3667 | 0.05 с |\n",
        "| **MLPClassifier (sklearn)** | Классификация | Accuracy = 0.9333 | 0.05 с |\n",
        "| **Keras (TensorFlow)** | Классификация | Accuracy = 1.0000 | 7.01 с |\n",
        "| **PyTorch** | Классификация | Accuracy = 0.9667 | 4.32 с |\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Регрессия (California Housing)\n",
        "\n",
        "### MyMLP:\n",
        "- RMSE: **0.8908**\n",
        "- Время: **37.54 секунды**\n",
        "\n",
        "### sklearn:\n",
        "- RMSE: **0.5766**\n",
        "- Время: **3.06 секунды**\n",
        "\n",
        "- `MyMLP` уступает по качеству и скорости `MLPRegressor` из `sklearn`\n",
        "\n",
        "---\n",
        "\n",
        "# Классификация (Iris)\n",
        "\n",
        "### MyMLP:\n",
        "- Accuracy: **0.3667** (почти случайное угадывание для 3 классов!)\n",
        "- Время: **0.05 с**\n",
        "\n",
        "### sklearn:\n",
        "- Accuracy: **0.9333**\n",
        "- Время: **0.05 с**\n",
        "\n",
        "### TensorFlow/Keras:\n",
        "- Accuracy: **1.0000**\n",
        "- Время: **7.01 с**\n",
        "\n",
        "### PyTorch:\n",
        "- Accuracy: **0.9667**\n",
        "- Время: **4.32 с**\n",
        "\n",
        "\n",
        "`MyMLP` обучается хуже чем внешние фреймворки\n",
        "\n",
        "---\n",
        "\n",
        "# Выводы\n",
        "\n",
        "В ходе выполнения лабораторной работы была разработана и протестирована полносвязная нейронная сеть `MyMLP`, реализованная с использованием только библиотеки `NumPy`. Модель была обучена на задачах регрессии (`California Housing`) и многоклассовой классификации (`Iris`).  \n",
        "\n",
        "\n",
        "Результаты показали, что:\n",
        "\n",
        "- Реализация `MyMLP` позволяет запустить обучение, но требует доработки для повышения качества.\n",
        "- Фреймворки `scikit-learn`, `TensorFlow`, `Keras`, `PyTorch` показывают значительно более высокую скорость и качество.\n",
        "\n",
        "Кастомный класс для полносвязной нейронной сети можно улучшить, если:\n",
        "\n",
        "- Улучшить инициализацию весов\n",
        "- Расширить набор поддерживаемых функций активации и лоссов\n",
        "- Добавить поддержку современных оптимизаторов (Adam, Momentum)\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "KQc5-9uxIl1b"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XgqwgxD8JtOV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}