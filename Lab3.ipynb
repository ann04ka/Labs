{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ann04ka/Labs/blob/main/Lab3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9QLe_T6GZUd"
      },
      "source": [
        "# RL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYlIf2yHv8hz"
      },
      "source": [
        "**Выполнять задание следует с текущими значениями гиперпараметров. Для проверки ниже будут приведены ответы, которые должны получиться в результате выполнения задания.**\n",
        "\n",
        "После того, как заявленные значения совпадут и будут получены работающие модели выполните следующие задания:\n",
        "\n",
        "1. **Базовая часть (3 балла):** исследуйте влияние параметров `gamma, lr_rate, epsilon` на обучение моделей. Постройте графики.\n",
        "2. **Опциональная часть (+2 балла):** Реализуйте остальные стратегии выбора действий (softmax, UCB, оптимистичные начальные оценки), описанные в лекции. Визуализируйте результаты."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDQzNIZXAoFE"
      },
      "source": [
        "Зададим гиперпараметры модели"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOMw2ZbOAmOZ"
      },
      "source": [
        "epsilon = 0.1 # Параметр эпсилон при использовании эпсилон жадной стратегии\n",
        "gamma = 0.8 # Коэффциент дисконтирования гамма\n",
        "random_seed = 100 #Random seed\n",
        "time_delay = 1 # Задержка времени при отрисовке процесса игры после обучения (секунды)\n",
        "lr_rate = 0.9 #Коэффициент скорости обучения альфа"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQu5IYHX8jId"
      },
      "source": [
        "Импортируем библиотеки, создаем свою среду размера 6х6. S обозначает точку старта. F -- лед безопасен, H -- проталина, G -- цель. Параметр `is_slippery=False` отвечает за условное отсутствие скольжения. То есть если агент выбрал действие пойти направо, то он переместится в соответствующее состояние. В общем случае из-за \"скольжения\" можно оказаться в другом состоянии. Мы также скопировали из библиотки GYM и слегка модифицировали функцию ```generate_random_map ```, для того, чтобы генерировать произвольные карты на основе ```random_seed ```.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Установим нужную версию библиотеки gym\n",
        "!git clone https://github.com/dvolchek/gym_0_18_0.git -q\n",
        "%cd /content/gym_0_18_0\n",
        "!pip install -e. -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjKH2tVSK2m0",
        "outputId": "616f7b58-7f0a-4e3f-d00e-8a98f6234026"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gym_0_18_0\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awL7CCCwD6C3",
        "outputId": "dbdc68fe-6cf0-452e-fcc1-161e6b8e2920",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import time\n",
        "from IPython.display import clear_output\n",
        "\n",
        "\n",
        "def generate_random_map(size, p, sd):\n",
        "    \"\"\"Generates a random valid map (one that has a path from start to goal)\n",
        "    :param size: size of each side of the grid\n",
        "    :param p: probability that a tile is frozen\n",
        "    \"\"\"\n",
        "    valid = False\n",
        "    np.random.seed(sd)\n",
        "\n",
        "    # DFS to check that it's a valid path.\n",
        "    def is_valid(res):\n",
        "        frontier, discovered = [], set()\n",
        "        frontier.append((0,0))\n",
        "        while frontier:\n",
        "            r, c = frontier.pop()\n",
        "            if not (r,c) in discovered:\n",
        "                discovered.add((r,c))\n",
        "                directions = [(1, 0), (0, 1), (-1, 0), (0, -1)]\n",
        "                for x, y in directions:\n",
        "                    r_new = r + x\n",
        "                    c_new = c + y\n",
        "                    if r_new < 0 or r_new >= size or c_new < 0 or c_new >= size:\n",
        "                        continue\n",
        "                    if res[r_new][c_new] == 'G':\n",
        "                        return True\n",
        "                    if (res[r_new][c_new] not in '#H'):\n",
        "                        frontier.append((r_new, c_new))\n",
        "        return False\n",
        "\n",
        "    while not valid:\n",
        "        p = min(1, p)\n",
        "        res = np.random.choice(['F', 'H'], (size, size), p=[p, 1-p])\n",
        "        res[0][0] = 'S'\n",
        "        res[-1][-1] = 'G'\n",
        "        valid = is_valid(res)\n",
        "    return [\"\".join(x) for x in res]\n",
        "\n",
        "#Генерация карты\n",
        "random_map = generate_random_map(size=6, p=0.8, sd = random_seed) #Создаем свою карту\n",
        "env = gym.make(\"FrozenLake-v0\", desc=random_map, is_slippery=False) #Инициализируем среду\n",
        "print(\"Ваша карта\")\n",
        "env.render() #Выводим карту на экран"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ваша карта\n",
            "\n",
            "\u001b[41mS\u001b[0mFFHFF\n",
            "FHFFHF\n",
            "FFFHHF\n",
            "HFFHHF\n",
            "FFFFFF\n",
            "FFFFFG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDCexoEU9a_c"
      },
      "source": [
        "Функции выбора действия и обновления таблицы ценности действий. Строчка *** используется для того, чтобы проверять ответы в openedx. Вне рамках академической задачи лучше использовать оригинальный метод класса `environment`, то есть:\n",
        "\n",
        "`action = env.action_space.sample()`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5TbDqn6G_Pt"
      },
      "source": [
        "# Задача 1\n",
        "Дополните функцию ```learn()```, чтобы в результате ее вызова обновлялось значение ценности текущего действия согласно алгоритму Q-обучения\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdQBpxaTOK7u"
      },
      "source": [
        "def choose_action(state):\n",
        "    action=0\n",
        "    if np.random.uniform(0, 1) < epsilon:\n",
        "        action = np.random.randint(0,env.action_space.n) #***\n",
        "    else:\n",
        "        action = np.random.choice(np.array(np.argwhere(Q[state, :] == np.amax(Q[state, :])).flatten().tolist()))\n",
        "    return action\n",
        "\n",
        "def learn(state, action, reward, state2, action2, done):\n",
        "    if done:\n",
        "        target = reward\n",
        "    else:\n",
        "        target = reward + gamma * Q[state2, action2]\n",
        "\n",
        "    Q[state, action] = Q[state, action] + lr_rate * (\n",
        "        target - Q[state, action]\n",
        "    )"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7COGeyA_Ist3"
      },
      "source": [
        "# Задача 2\n",
        "Дополните следующий код так, чтобы в результате обучения модели можно было узнать количество побед и номер игры (`game`), на котором агент впервые одержал пятую победу подряд."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0adDl7NvJoQP"
      },
      "source": [
        "Поясним, что возвращает функция ```env.step(action)```\n",
        "\n",
        "```state2``` -- следующее состояние\n",
        "\n",
        "```reward``` -- награда\n",
        "\n",
        "```done``` -- флаг окончания игры. True в случае победы или падения в проталину. False в остальных случаях.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aq92-dWiOchF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d029c980-b6a0-4341-ffb5-db28723f541e"
      },
      "source": [
        "from tqdm import tqdm\n",
        "# Inititalization\n",
        "np.random.seed(random_seed)\n",
        "total_games = 10000\n",
        "max_steps = 100\n",
        "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "\n",
        "wins = []\n",
        "five_in_a_row = -1\n",
        "recent_wins = [0] * 5\n",
        "\n",
        "#Main cycle\n",
        "for game in tqdm(range(total_games)):\n",
        "    state = env.reset()\n",
        "    t = 0\n",
        "    won = 0\n",
        "    while t < max_steps:\n",
        "        t += 1\n",
        "        action = choose_action(state)\n",
        "        state2, reward, done, info = env.step(action)\n",
        "\n",
        "        if t == max_steps:\n",
        "            done = True\n",
        "\n",
        "        learn(state, state2, reward, action, done)\n",
        "\n",
        "        state = state2\n",
        "\n",
        "        if reward == 1.0:\n",
        "            won = 1\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    wins.append(won)\n",
        "\n",
        "    recent_wins.append(won)\n",
        "    recent_wins.pop(0)\n",
        "\n",
        "    if sum(recent_wins) == 5 and five_in_a_row == -1:\n",
        "        five_in_a_row = game + 1"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [00:07<00:00, 1354.02it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFuxsqdRLOS9"
      },
      "source": [
        "Вывод ответов при заданных параметрах"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZbJtFnhLa7w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2123758-f5fd-4f61-e681-67f4b269bf20"
      },
      "source": [
        "print(\"Количество побед в серии из 10 000 игр:\", sum(wins))\n",
        "print(\"Пять побед подряд впервые было одержано в игре:\", five_in_a_row)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Количество побед в серии из 10 000 игр: 7914\n",
            "Пять побед подряд впервые было одержано в игре: 885\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSXdSiG2WI71"
      },
      "source": [
        "Должны получиться следующие результаты.\n",
        "\n",
        "\n",
        "*  Количество побед в серии из 10 000 игр:  7914\n",
        "*  Пять побед подряд впервые было одержано в игре  885\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nazZaAbwQGBt"
      },
      "source": [
        "Произведем одну игру, чтобы проследить за действиями агента. При этом будем считать модель полностью обученной, то есть действия выбираются жадно, значения ценностей действий в таблице не обновляются."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ysllZjEQXLa",
        "outputId": "0ad493e7-6510-4820-bf07-ea57611be613",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import time\n",
        "#Жадный выбор действий\n",
        "def choose_action_one_game(state):\n",
        "    action = np.random.choice(np.array(np.argwhere(Q[state, :] == np.amax(Q[state, :])).flatten().tolist()))\n",
        "    return action\n",
        "\n",
        "states=[]#Массив для сохранения состояний агента в течение игры\n",
        "t = 0\n",
        "state = env.reset()\n",
        "wn = 0\n",
        "while(t<100):\n",
        "  env.render()\n",
        "  time.sleep(time_delay)\n",
        "  clear_output(wait=True)\n",
        "  action = choose_action_one_game(state)\n",
        "  state2, reward, done, info = env.step(action)\n",
        "  states.append(state)\n",
        "  state = state2\n",
        "  t += 1\n",
        "  if done and reward == 1:\n",
        "    wn=1\n",
        "  if done:\n",
        "    break\n",
        "if wn == 1:\n",
        "  print(\"!!!Победа!!!\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!!Победа!!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x696NulpReFI"
      },
      "source": [
        "Отобразим маршрут"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKMCMdpOTcXy",
        "outputId": "a21bd5f9-ee8e-4bc0-83c6-48c5cf71696b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def make_maze_pic(maze):\n",
        "  maze_pic=[]\n",
        "  for i in range(len(maze)):\n",
        "    row = []\n",
        "    for j in range(len(maze[i])):\n",
        "      if maze[i][j] == 'S':\n",
        "        row.append(0)\n",
        "      if maze[i][j] == 'F':\n",
        "        row.append(0)\n",
        "      if maze[i][j] == 'H':\n",
        "        row.append(1)\n",
        "      if maze[i][j] == 'G':\n",
        "        row.append(0)\n",
        "    maze_pic.append(row)\n",
        "  maze_pic = np.array(maze_pic)\n",
        "  return maze_pic\n",
        "\n",
        "\n",
        "#Make maze fit to plot\n",
        "maze_pic = make_maze_pic(random_map)\n",
        "nrows, ncols = maze_pic.shape\n",
        "\n",
        "#Arrays of picture elements\n",
        "rw = np.remainder(states,nrows)\n",
        "cl = np.floor_divide(states,nrows)\n",
        "if wn == 1:\n",
        "  rw = np.append(rw, [nrows-1])\n",
        "  cl = np.append(cl,[ncols-1])\n",
        "\n",
        "#Picture plotting\n",
        "fig, ax1 = plt.subplots(1, 1, tight_layout=True)\n",
        "ax1.clear()\n",
        "ax1.set_xticks(np.arange(0.5, nrows, step=1))\n",
        "ax1.set_xticklabels([])\n",
        "ax1.set_yticks(np.arange(0.5, ncols, step=1))\n",
        "ax1.set_yticklabels([])\n",
        "ax1.grid(True)\n",
        "ax1.plot([0],[0], \"gs\", markersize=40)  # start is a big green square\n",
        "ax1.text(0, 0.2,\"Start\", ha=\"center\", va=\"center\", color=\"white\", fontsize=12) #Start text\n",
        "ax1.plot([nrows-1],[ncols-1], \"rs\", markersize=40)  # exit is a big red square\n",
        "ax1.text(nrows-1, ncols-1+0.2,\"Finish\", ha=\"center\", va=\"center\", color=\"white\", fontsize=12) #Exit text\n",
        "ax1.plot(rw,cl, ls = '-', color = 'blue') #Blue lines path\n",
        "ax1.plot(rw,cl, \"bo\")  # Blue dots visited cells\n",
        "ax1.imshow(maze_pic, cmap=\"binary\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7884f21bddd0>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5m14YFyrI6M0"
      },
      "source": [
        "# Задача 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lb3BvDuBxTO0"
      },
      "source": [
        "Используйте вместо алгоритма Q-обучения алгоритм SARSA. Обратите внимание на то, что требуется изменить количество игр. То есть `total_games = 40000`. Запускать блоки следует последвательно с самого начала (из-за `random_seed`). Отдельно обращаем ваше внимание на то, что при изменении алгоритма с Q-обучения на SARSA модификации подлежит как процесс обучения, так и функция `learn()`. Кроме того, у функции `learn()` должен появиться дополнительный аргумент (следующее действие). Ниже приведен фрагмент кода с пояснениями, как именно нужно модифицировать алгоритм.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSVpTwTAJO7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60e0c4be-eae0-4009-df67-0a51c0665460"
      },
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(random_seed)\n",
        "total_games = 40000\n",
        "max_steps = 100\n",
        "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "\n",
        "wins = []\n",
        "five_in_a_row = -1\n",
        "recent_wins = [0] * 5\n",
        "\n",
        "# Main cycle\n",
        "for game in tqdm(range(total_games)):\n",
        "    state = env.reset()\n",
        "    t = 0\n",
        "    action = choose_action(state)\n",
        "    won = 0\n",
        "\n",
        "    while t < max_steps:\n",
        "        t += 1\n",
        "\n",
        "        state2, reward, done, info = env.step(action)\n",
        "\n",
        "        action2 = choose_action(state2)\n",
        "\n",
        "        if t == max_steps:\n",
        "            done = True\n",
        "\n",
        "        learn(state, action, reward, state2, action2, done)\n",
        "\n",
        "        state = state2\n",
        "        action = action2\n",
        "\n",
        "        if reward == 1.0:\n",
        "            won = 1\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    wins.append(won)\n",
        "\n",
        "    recent_wins.append(won)\n",
        "    recent_wins.pop(0)\n",
        "\n",
        "    if sum(recent_wins) == 5 and five_in_a_row == -1:\n",
        "        five_in_a_row = game + 1"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40000/40000 [00:32<00:00, 1245.41it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Количество побед в серии из 10 000 игр:\", sum(wins))\n",
        "print(\"Пять побед подряд впервые было одержано в игре:\", five_in_a_row)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGzm_2S3PNNN",
        "outputId": "d3466d20-7d09-4082-8b8d-97652be95714"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Количество побед в серии из 10 000 игр: 32328\n",
            "Пять побед подряд впервые было одержано в игре: 894\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RB_PX2vYIY0-"
      },
      "source": [
        ". В результате обучения должны получиться следующие ответы:\n",
        "\n",
        "\n",
        "\n",
        "*   Количество побед в серии из 40 000 игр:  32328\n",
        "*   Пять побед подряд впервые было одержано в игре  894"
      ]
    }
  ]
}